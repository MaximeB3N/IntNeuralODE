{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils.utils import add_spatial_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Generating images...\n",
      "(10000, 20, 28, 28) 1.0 0.0\n",
      "(200000, 1, 28, 28)\n",
      "torch.Size([8000, 20, 3, 28, 28]) torch.Size([2000, 20, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "MARGIN_MIN = 7\n",
    "MIN_INIT_VELOCITY = 1000.\n",
    "WIDTH, HEIGHT = 28, 28\n",
    "RADIUS = 3\n",
    "size = WIDTH\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Generating images...\")\n",
    "images = np.load(\"data/movingmnist_dataset_10000_20_1_28_2/sequences.npy\").swapaxes(-1, -2)\n",
    "# Apply the sigmoid function to the images\n",
    "# images = np.exp(3*images) / (1 + np.exp(3*images)) - 0.5\n",
    "images = (images - images.min()) / (images.max() - images.min())\n",
    "# images = np.load(\"data/mnist_test_seq_resized.npy\").swapaxes(0,1)\n",
    "print(images.shape, images.max(), images.min())\n",
    "shape = images.shape\n",
    "\n",
    "images = images.reshape(-1, 1, shape[-2], shape[-1])\n",
    "print(images.shape)\n",
    "N = shape[0]\n",
    "N_frames = shape[1]\n",
    "dt = 1./N_frames\n",
    "Num_pos_velocity = 1\n",
    "times = np.arange(0, N_frames*dt, dt)\n",
    "\n",
    "# dataset = [(image, 0) for image in dataset]\n",
    "# dataset = add_spatial_encoding(dataset)\n",
    "# print(len(dataset), len(dataset[0]), dataset[0][0].shape)\n",
    "images = torch.from_numpy(add_spatial_encoding(images)).float().reshape(N, -1, 3, HEIGHT, WIDTH)\n",
    "\n",
    "# split the dataset\n",
    "images_train, images_test = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "print(images_train.shape, images_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNR:\n",
    "    \"\"\"Peak Signal to Noise Ratio\n",
    "    img1 and img2 have range [0, 1]\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"PSNR\"\n",
    "        self._value = 0.0\n",
    "        self._num_examples = 0\n",
    "\n",
    "    \n",
    "    def update(self, img1, img2):\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Input images must have the same dimensions but got %s and %s\" % (str(img1.shape), str(img2.shape)))\n",
    "        \n",
    "        if img1.ndim !=4 and img1.ndim != 5:\n",
    "            raise ValueError(\"Input images must have 4 or 5 dimensions but got {}\".format(img1.ndim))\n",
    "            \n",
    "        if img1.ndim == 5 and img2.ndim == 5:\n",
    "            img1 = img1.view(-1, *img1.shape[2:])\n",
    "            img2 = img2.view(-1, *img2.shape[2:])\n",
    "        \n",
    "        mse = torch.mean((img1 - img2)**2, dim=(1, 2, 3))\n",
    "\n",
    "        self._value += 10 * torch.log10(1. / mse).sum()\n",
    "        self._num_examples += img1.shape[0]\n",
    "\n",
    "    def reset(self):\n",
    "        self._value = 0.0\n",
    "        self._num_examples = 0\n",
    "\n",
    "\n",
    "    def compute(self):\n",
    "        if self._num_examples == 0:\n",
    "            raise ValueError(\"SSIM must have at least one example before it can be computed.\")\n",
    "        return self._value / self._num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence, Callable\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes Structual Similarity Index Measure\n",
    "    Args:\n",
    "        data_range: Range of the image. Typically, ``1.0`` or ``255``.\n",
    "        kernel_size: Size of the kernel. Default: (11, 11)\n",
    "        sigma: Standard deviation of the gaussian kernel.\n",
    "            Argument is used if ``gaussian=True``. Default: (1.5, 1.5)\n",
    "        k1: Parameter of SSIM. Default: 0.01\n",
    "        k2: Parameter of SSIM. Default: 0.03\n",
    "        gaussian: ``True`` to use gaussian kernel, ``False`` to use uniform kernel\n",
    "        output_transform: A callable that is used to transform the\n",
    "            :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the\n",
    "            form expected by the metric.\n",
    "        device: specifies which device updates are accumulated on. Setting the metric's\n",
    "            device to be the same as your ``update`` arguments ensures the ``update`` method is non-blocking. By\n",
    "            default, CPU.\n",
    "    Examples:\n",
    "        To use with ``Engine`` and ``process_function``, simply attach the metric instance to the engine.\n",
    "        The output of the engine's ``process_function`` needs to be in the format of\n",
    "        ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y, ...}``.\n",
    "        ``y_pred`` and ``y`` can be un-normalized or normalized image tensors. Depending on that, the user might need\n",
    "        to adjust ``data_range``. ``y_pred`` and ``y`` should have the same shape.\n",
    "        For more information on how metric works with :class:`~ignite.engine.engine.Engine`, visit :ref:`attach-engine`.\n",
    "        .. include:: defaults.rst\n",
    "            :start-after: :orphan:\n",
    "        .. testcode::\n",
    "            metric = SSIM(data_range=1.0)\n",
    "            metric.attach(default_evaluator, 'ssim')\n",
    "            preds = torch.rand([4, 3, 16, 16])\n",
    "            target = preds * 0.75\n",
    "            state = default_evaluator.run([[preds, target]])\n",
    "            print(state.metrics['ssim'])\n",
    "        .. testoutput::\n",
    "            0.9218971...\n",
    "    .. versionadded:: 0.4.2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range: Union[int, float],\n",
    "        kernel_size: Union[int, Sequence[int]] = (11, 11),\n",
    "        sigma: Union[float, Sequence[float]] = (1.5, 1.5),\n",
    "        k1: float = 0.01,\n",
    "        k2: float = 0.03,\n",
    "        gaussian: bool = True,\n",
    "        output_transform: Callable = lambda x: x,\n",
    "        device: Union[str, torch.device] = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = [kernel_size, kernel_size]  # type: Sequence[int]\n",
    "        elif isinstance(kernel_size, Sequence):\n",
    "            self.kernel_size = kernel_size\n",
    "        else:\n",
    "            raise ValueError(\"Argument kernel_size should be either int or a sequence of int.\")\n",
    "\n",
    "        if isinstance(sigma, float):\n",
    "            self.sigma = [sigma, sigma]  # type: Sequence[float]\n",
    "        elif isinstance(sigma, Sequence):\n",
    "            self.sigma = sigma\n",
    "        else:\n",
    "            raise ValueError(\"Argument sigma should be either float or a sequence of float.\")\n",
    "\n",
    "        if any(x % 2 == 0 or x <= 0 for x in self.kernel_size):\n",
    "            raise ValueError(f\"Expected kernel_size to have odd positive number. Got {kernel_size}.\")\n",
    "\n",
    "        if any(y <= 0 for y in self.sigma):\n",
    "            raise ValueError(f\"Expected sigma to have positive number. Got {sigma}.\")\n",
    "\n",
    "        super(SSIM, self).__init__()\n",
    "        self._device = device\n",
    "        self._output_transform = output_transform\n",
    "        self.gaussian = gaussian\n",
    "        self.c1 = (k1 * data_range) ** 2\n",
    "        self.c2 = (k2 * data_range) ** 2\n",
    "        self.pad_h = (self.kernel_size[0] - 1) // 2\n",
    "        self.pad_w = (self.kernel_size[1] - 1) // 2\n",
    "        self._kernel = self._gaussian_or_uniform_kernel(kernel_size=self.kernel_size, sigma=self.sigma)\n",
    "        self.reset()\n",
    "        \n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._sum_of_ssim = torch.tensor(0.0, dtype=torch.float64, device=self._device)\n",
    "        self._num_examples = 0\n",
    "        self._kernel = self._gaussian_or_uniform_kernel(kernel_size=self.kernel_size, sigma=self.sigma)\n",
    "\n",
    "    def _uniform(self, kernel_size: int) -> torch.Tensor:\n",
    "        max, min = 2.5, -2.5\n",
    "        ksize_half = (kernel_size - 1) * 0.5\n",
    "        kernel = torch.linspace(-ksize_half, ksize_half, steps=kernel_size, device=self._device)\n",
    "        for i, j in enumerate(kernel):\n",
    "            if min <= j <= max:\n",
    "                kernel[i] = 1 / (max - min)\n",
    "            else:\n",
    "                kernel[i] = 0\n",
    "\n",
    "        return kernel.unsqueeze(dim=0)  # (1, kernel_size)\n",
    "\n",
    "    def _gaussian(self, kernel_size: int, sigma: float) -> torch.Tensor:\n",
    "        ksize_half = (kernel_size - 1) * 0.5\n",
    "        kernel = torch.linspace(-ksize_half, ksize_half, steps=kernel_size, device=self._device)\n",
    "        gauss = torch.exp(-0.5 * (kernel / sigma).pow(2))\n",
    "        return (gauss / gauss.sum()).unsqueeze(dim=0)  # (1, kernel_size)\n",
    "\n",
    "    def _gaussian_or_uniform_kernel(self, kernel_size: Sequence[int], sigma: Sequence[float]) -> torch.Tensor:\n",
    "        if self.gaussian:\n",
    "            kernel_x = self._gaussian(kernel_size[0], sigma[0])\n",
    "            kernel_y = self._gaussian(kernel_size[1], sigma[1])\n",
    "        else:\n",
    "            kernel_x = self._uniform(kernel_size[0])\n",
    "            kernel_y = self._uniform(kernel_size[1])\n",
    "\n",
    "        return torch.matmul(kernel_x.t(), kernel_y)  # (kernel_size, 1) * (1, kernel_size)\n",
    "\n",
    "    def update(self, y_pred, y) -> None:\n",
    "        y_pred, y = y_pred.detach(), y.detach()\n",
    "\n",
    "        if y_pred.dtype != y.dtype:\n",
    "            raise TypeError(\n",
    "                f\"Expected y_pred and y to have the same data type. Got y_pred: {y_pred.dtype} and y: {y.dtype}.\"\n",
    "            )\n",
    "\n",
    "        if y_pred.shape != y.shape:\n",
    "            raise ValueError(\n",
    "                f\"Expected y_pred and y to have the same shape. Got y_pred: {y_pred.shape} and y: {y.shape}.\"\n",
    "            )\n",
    "\n",
    "        if len(y_pred.shape) == 5 and len(y.shape) == 5:\n",
    "            y_pred = y_pred.view(-1, *y_pred.shape[2:])\n",
    "            y = y.view(-1, *y.shape[2:])\n",
    "            \n",
    "        if len(y_pred.shape) != 4 or len(y.shape) != 4:\n",
    "            raise ValueError(\n",
    "                f\"Expected y_pred and y to have BxCxHxW shape. Got y_pred: {y_pred.shape} and y: {y.shape}.\"\n",
    "            )\n",
    "\n",
    "        channel = y_pred.size(1)\n",
    "        if len(self._kernel.shape) < 4:\n",
    "            self._kernel = self._kernel.expand(channel, 1, -1, -1).to(device=y_pred.device)\n",
    "\n",
    "        y_pred = F.pad(y_pred, [self.pad_w, self.pad_w, self.pad_h, self.pad_h], mode=\"reflect\")\n",
    "        y = F.pad(y, [self.pad_w, self.pad_w, self.pad_h, self.pad_h], mode=\"reflect\")\n",
    "\n",
    "        input_list = torch.cat([y_pred, y, y_pred * y_pred, y * y, y_pred * y])\n",
    "        outputs = F.conv2d(input_list, self._kernel, groups=channel)\n",
    "\n",
    "        output_list = [outputs[x * y_pred.size(0) : (x + 1) * y_pred.size(0)] for x in range(len(outputs))]\n",
    "\n",
    "        mu_pred_sq = output_list[0].pow(2)\n",
    "        mu_target_sq = output_list[1].pow(2)\n",
    "        mu_pred_target = output_list[0] * output_list[1]\n",
    "\n",
    "        sigma_pred_sq = output_list[2] - mu_pred_sq\n",
    "        sigma_target_sq = output_list[3] - mu_target_sq\n",
    "        sigma_pred_target = output_list[4] - mu_pred_target\n",
    "\n",
    "        a1 = 2 * mu_pred_target + self.c1\n",
    "        a2 = 2 * sigma_pred_target + self.c2\n",
    "        b1 = mu_pred_sq + mu_target_sq + self.c1\n",
    "        b2 = sigma_pred_sq + sigma_target_sq + self.c2\n",
    "\n",
    "        ssim_idx = (a1 * a2) / (b1 * b2)\n",
    "        self._sum_of_ssim += torch.mean(ssim_idx, (1, 2, 3), dtype=torch.float64).sum().to(self._device)\n",
    "\n",
    "        self._num_examples += y.shape[0]\n",
    "\n",
    "        \n",
    "\n",
    "    def compute(self) -> float:\n",
    "        if self._num_examples == 0:\n",
    "            raise ValueError(\"SSIM must have at least one example before it can be computed.\")\n",
    "        return (self._sum_of_ssim / self._num_examples).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PSNR()\n",
    "ssim = SSIM(data_range=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.7956)\n"
     ]
    }
   ],
   "source": [
    "preds = torch.rand([4, 3, 16, 16])\n",
    "target = preds * 0.75\n",
    "psnr.update(preds, target)\n",
    "print(psnr.compute())\n",
    "psnr.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "0.6637754018690903\n",
      "tensor(14.2807) 0.6400145810351162\n",
      "tensor(14.2807) 0.6400145810351161\n"
     ]
    }
   ],
   "source": [
    "trajectory = images[:, 0, 0, :, :].unsqueeze(1)\n",
    "print(trajectory.shape)\n",
    "trajectory2 = images[:, 4, 0, :, :].unsqueeze(1)\n",
    "print(trajectory2.shape)\n",
    "ssim.update(trajectory, trajectory2)\n",
    "print(ssim.compute())\n",
    "ssim.reset()\n",
    "\n",
    "# trajectory = images[:, :, 0, :, :].unsqueeze(2)\n",
    "# print(trajectory.shape)\n",
    "# trajectory2 = images[:, :, 0, :, :].unsqueeze(2)\n",
    "# print(trajectory2.shape)\n",
    "# print(ssim(trajectory, trajectory2))\n",
    "\n",
    "for i in range(10):\n",
    "    trajectory = images[i, :, 0, :, :].unsqueeze(1)\n",
    "    trajectory2 = images[i+1, :, 0, :, :].unsqueeze(1)\n",
    "    psnr.update(trajectory, trajectory2)\n",
    "    ssim.update(trajectory, trajectory2)\n",
    "\n",
    "print(psnr.compute(), ssim.compute())\n",
    "psnr.reset()\n",
    "ssim.reset()\n",
    "\n",
    "trajectory = images[:10, :, 0, :, :].unsqueeze(2)\n",
    "trajectory2 = images[1:11, :, 0, :, :].unsqueeze(2)\n",
    "psnr.update(trajectory, trajectory2)\n",
    "ssim.update(trajectory, trajectory2)\n",
    "print(psnr.compute(), ssim.compute())\n",
    "psnr.reset()\n",
    "ssim.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ignite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b9e457389096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpsnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPSNR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ignite' is not defined"
     ]
    }
   ],
   "source": [
    "psnr = ignite.metrics.PSNR(data_range=1.)\n",
    "ssim = ignite.metrics.SSIM(data_range=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift = 5\n",
    "test_input = images_test[0, :, :, :, :]\n",
    "test_target = images_test[shift, :, :, :, :]\n",
    "\n",
    "index = 0\n",
    "plt.imshow(test_input[index, 0].numpy())\n",
    "plt.show()\n",
    "plt.imshow(test_target[index, 0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test between 2 diff trajectories\n",
    "ssim.attach(default_evaluator, 'ssim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected y_pred and y to have BxCxHxW shape. Got y_pred: torch.Size([28, 28]) and y: torch.Size([28, 28]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8f4a47637178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PSNR:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SSIM:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.6/site-packages/ignite/metrics/metric.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.6/site-packages/ignite/metrics/ssim.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             raise ValueError(\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;34mf\"Expected y_pred and y to have BxCxHxW shape. Got y_pred: {y_pred.shape} and y: {y.shape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             )\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected y_pred and y to have BxCxHxW shape. Got y_pred: torch.Size([28, 28]) and y: torch.Size([28, 28])."
     ]
    }
   ],
   "source": [
    "# evaluate ssim and psnr\n",
    "psnr.reset()\n",
    "ssim.reset()\n",
    "psnr.update((test_input[index, 0].unsqueeze(1), test_target[index, 0].unsqueeze(1)))\n",
    "ssim.update((test_input[index, 0], test_target[index, 0]))\n",
    "print(\"PSNR:\", psnr.compute())\n",
    "print(\"SSIM:\", ssim.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
