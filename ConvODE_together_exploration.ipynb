{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact\n",
    "\n",
    "from src.data.box import GravityHoleBall\n",
    "from src.data.generate import generate_gravity_hole_ball_images\n",
    "\n",
    "from src.utils.utils import add_spatial_encoding\n",
    "from src.utils.node import  BatchGetterMultiImages, train_convnode_with_batch\n",
    "from src.utils.viz import  display_convnode_trajectory\n",
    "\n",
    "from src.models.convnode import ConvNodeWithBatch#, LatentRegularizerLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Generating images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 28.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 1, 28, 28)\n",
      "torch.Size([3, 301, 3, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MARGIN_MIN = 5\n",
    "MIN_INIT_VELOCITY = 200.\n",
    "WIDTH, HEIGHT = 28, 28\n",
    "RADIUS = 3\n",
    "\n",
    "infos = {\n",
    "    \"MARGIN_MIN\":MARGIN_MIN,\n",
    "    \"MIN_INIT_VELOCITY\":MIN_INIT_VELOCITY,\n",
    "    \"WIDTH\":WIDTH,\n",
    "    \"HEIGHT\":HEIGHT,\n",
    "    \"RADIUS\":RADIUS\n",
    "}\n",
    "\n",
    "x = WIDTH/4.\n",
    "y = HEIGHT/4.\n",
    "vx = 0.\n",
    "vy = 0.\n",
    "\n",
    "box = GravityHoleBall(x, y, vx, vy, (WIDTH, HEIGHT),RADIUS)\n",
    "\n",
    "\n",
    "Num_pos_velocity = 1\n",
    "N = 3\n",
    "N_frames = 300 + Num_pos_velocity\n",
    "dt = 1./N_frames\n",
    "\n",
    "times = np.arange(0, N_frames*dt, dt)\n",
    "\n",
    "# encoded_trajectory = generate_gravity_hole_ball_positions(box, N=N, N_frames=N_frames, dt=dt)[:,:,:]\n",
    "# print(encoded_trajectory.shape)\n",
    "print(\"-\"*50)\n",
    "print(\"Generating images...\")\n",
    "images = generate_gravity_hole_ball_images(box, N=N, N_frames=N_frames, dt=dt, infos=infos).reshape(-1, 1, HEIGHT, WIDTH)\n",
    "print(images.shape)\n",
    "# dataset = [(image, 0) for image in dataset]\n",
    "# dataset = add_spatial_encoding(dataset)\n",
    "# print(len(dataset), len(dataset[0]), dataset[0][0].shape)\n",
    "images = torch.from_numpy(add_spatial_encoding(images)).float().reshape(N, -1, 3, HEIGHT, WIDTH)\n",
    "print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Creating model...\n",
      "--------------------------------------------------\n",
      "Creating ConvAE...\n",
      "Number of parameters in the encoder model: 146016\n",
      "Number of parameters in the decoder model: 146147\n",
      "--------------------------------------------------\n",
      "Creating ANODENet...\n",
      "Number of parameters in the model: 35168\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Creating model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "size = 28\n",
    "latent_dim = 32\n",
    "in_channels = 3\n",
    "ode_data_dim = latent_dim\n",
    "ode_hidden_dim = 128\n",
    "augment_dim=0\n",
    "time_dependent=False\n",
    "ode_non_linearity='relu' \n",
    "conv_activation=nn.ReLU()\n",
    "latent_activation=None\n",
    "stack_size=1\n",
    "\n",
    "conv_ode = ConvNodeWithBatch(device, size, latent_dim, in_channels,\n",
    "    ode_hidden_dim, ode_data_dim, augment_dim=augment_dim, time_dependent=time_dependent,\n",
    "    ode_non_linearity=ode_non_linearity, ode_linear_layer=False, conv_activation=conv_activation,\n",
    "    latent_activation=latent_activation, stack_size=stack_size)\n",
    "\n",
    "\n",
    "# pathConvODE = \"models/AE_ODE/ConvODE/conv_ode_1_ball_latent_{}_hidden_ode_{}_stack_{}_conv_activation_{}_with_lambda_decay.pt\".format(latent_dim, ode_hidden_dim, stack_size, conv_activation)\n",
    "# pathConvODE = \"ssh/conv_ode_1_ball_latent_32_hidden_ode_128_stack_1_conv_activation_ReLU()_with_lambda_decay_together.pt\"\n",
    "pathConvODE = \"ssh/conv_ode_1_ball_latent_32_hidden_ode_128_stack_1_conv_activation_ReLU()_with_lambda_decay_ODE_and_decoder.pt\"\n",
    "print(conv_ode.load_state_dict(torch.load(pathConvODE, map_location=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Creating tools to train...\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Creating tools to train...\")\n",
    "batch_size = 32\n",
    "batch_time = 100\n",
    "n_stack = 1\n",
    "total_length = N_frames - Num_pos_velocity\n",
    "getter = BatchGetterMultiImages(batch_time, batch_size, n_stack, total_length, dt, images.to(device), frac_train=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentRegularizerLoss(nn.Module):\n",
    "    def __init__(self, device, reg_lambda, step_decay=1, decay_rate=0.9):\n",
    "        super(LatentRegularizerLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.image_loss = nn.MSELoss()\n",
    "        self.step_decay = step_decay\n",
    "        self.decay_rate = decay_rate\n",
    "        self._step = 0\n",
    "\n",
    "    def forward(self, latent_z, pred_images, true_images):\n",
    "        # latent_z: [batch, latent_dim]\n",
    "        # pred_images: [batch, n_stack, in_channels, height, width]\n",
    "        # true_images: [batch, n_stack, in_channels, height, width]\n",
    "        loss_img = self.image_loss(pred_images, true_images)\n",
    "        loss_reg = torch.linalg.norm(latent_z, ord=2, dim=-1).mean(dim=-1).mean(dim=-1)\n",
    "        # print(\"loss_img: \", loss_img)\n",
    "        # print(\"loss_reg: \", loss_reg)\n",
    "        return loss_img + self.reg_lambda * loss_reg\n",
    "    \n",
    "\n",
    "    def step(self):\n",
    "        self._step +=1\n",
    "        if self._step % self.step_decay == 0:\n",
    "            self.reg_lambda *= self.decay_rate\n",
    "            \n",
    "\n",
    "    def forward_print(self, latent_z, pred_images, true_images):\n",
    "        # latent_z: [batch, latent_dim]\n",
    "        # pred_images: [batch, n_stack, in_channels, height, width]\n",
    "        # true_images: [batch, n_stack, in_channels, height, width]\n",
    "        loss_img = self.image_loss(pred_images, true_images)\n",
    "        loss_reg = torch.linalg.norm(latent_z, ord=2, dim=-1).mean(dim=-1).mean(dim=-1)\n",
    "        print(\"-\"*30, \"Loss prints\", \"-\"*30)\n",
    "        print(\"loss_img: \", loss_img)\n",
    "        print(\"loss_reg: \", self.reg_lambda * loss_reg)\n",
    "        print(\"reg_lambda: \",self.reg_lambda)\n",
    "        print(\"-\"*73)\n",
    "        return None\n",
    "\n",
    "reg_lambda = 0.0005\n",
    "optimizer = torch.optim.Adam(conv_ode.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.9)\n",
    "loss_fn = LatentRegularizerLoss(device, reg_lambda, step_decay=3000, decay_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = None #\"images/AE_ODE/Gravity/MultiTrajectories/Together/\"\n",
    "name = None # \"conv_ode_1_ball_latent_{}_hidden_ode_{}_stack_{}_conv_activation_{}\".format(latent_dim, ode_hidden_dim, stack_size, conv_activation)\n",
    "display_fn = lambda i, model, out_display, getter, final_time, dt: display_convnode_trajectory(i, model, out_display, getter, final_time, dt, root=root, name=name)\n",
    "display_fn(0, conv_ode, latent_dim, getter, N_frames - Num_pos_velocity, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Training...\")\n",
    "epochs = 10000\n",
    "train_convnode_with_batch(conv_ode, optimizer, scheduler, epochs,\n",
    "    getter, loss_fn=loss_fn, display=1000, display_results_fn=display_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathConvODE = \"conv_ode_1_ball_latent_{}_hidden_ode_{}_stack_{}_conv_activation_{}_with_lambda_decay_together.pt\".format(latent_dim, ode_hidden_dim, stack_size, conv_activation)\n",
    "torch.save(conv_ode.state_dict(), pathConvODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Training...\")\n",
    "for param in conv_ode.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "epochs = 10000\n",
    "train_convnode_with_batch(conv_ode, optimizer, scheduler, epochs,\n",
    "    getter, loss_fn=loss_fn, display=1000, display_results_fn=display_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathConvODE = \"conv_ode_1_ball_latent_{}_hidden_ode_{}_stack_{}_conv_activation_{}_with_lambda_decay_ODE_and_decoder.pt\".format(latent_dim, ode_hidden_dim, stack_size, conv_activation)\n",
    "torch.save(conv_ode.state_dict(), pathConvODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import gray2rgb\n",
    "\n",
    "def generate_interactive_plot(i, model, out_display, getter, final_time, dt, root=None, name=None):\n",
    "\n",
    "    index = np.random.randint(0, getter.N_train)\n",
    "    time_steps = np.linspace(0, final_time*dt, final_time)\n",
    "\n",
    "    times = torch.arange(0, final_time*dt, dt)\n",
    "\n",
    "    gd_images = getter.train_images[index, :-1][:,0].cpu().numpy()\n",
    "    input_images = getter.train_images[index, :2]\n",
    "    with torch.no_grad():\n",
    "        reconstructed_images, _ = model(input_images, times, dt)\n",
    "        reconstructed_images = reconstructed_images.cpu().numpy()\n",
    "\n",
    "    print(\"Sim\", reconstructed_images.shape, gd_images.shape)\n",
    "\n",
    "    gd_images = np.expand_dims(gd_images, axis=1)\n",
    "    reconstructed_images = np.expand_dims(reconstructed_images[:,0], axis=1)\n",
    "    print(\"Extract Gray\", gd_images.shape, reconstructed_images.shape)\n",
    "\n",
    "    gd_images = np.array([gray2rgb(img[0]) for img in gd_images])\n",
    "    gd_images = 200*(gd_images - gd_images.min())/(gd_images.max() - gd_images.min())\n",
    "    print(\"Gray to rgb for gd_images\", gd_images.shape, reconstructed_images.shape)\n",
    "\n",
    "    reconstructed_images = np.array([gray2rgb(img[0]) for img in reconstructed_images])\n",
    "    reconstructed_images = 200*(reconstructed_images - reconstructed_images.min())/(reconstructed_images.max() - reconstructed_images.min())\n",
    "\n",
    "    print(\"Gray to rgb for reconstructed\", gd_images.shape, reconstructed_images.shape)\n",
    "\n",
    "    return interactive_part_trajectory_image_plot(gd_images, reconstructed_images, time_steps, dt)\n",
    "\n",
    "\n",
    "def interactive_part_trajectory_image_plot(inputs_images, reconstructed_images, time_steps, dt):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Input image\", \"Predicted image\"))\n",
    "    fig = go.FigureWidget(fig)\n",
    "    # add a black background to the figure\n",
    "    fig.add_image(z=inputs_images[0], row=1, col=1, name='true image')\n",
    "    fig.add_image(z=reconstructed_images[0], row=1, col=2, name='predicted image')\n",
    "\n",
    "    N_max_input = len(inputs_images)-1\n",
    "    N_max_predicted = len(reconstructed_images)-1\n",
    "    N_max = max(N_max_input, N_max_predicted)\n",
    "\n",
    "    frac_input = 1. #N_max/N_max_predicted\n",
    "    frac_predicted = 1. #N_max/N_max_input\n",
    "\n",
    "    @interact(t=(time_steps.min(),time_steps.max(),dt))\n",
    "    def update_plot(t):\n",
    "        with fig.batch_update():\n",
    "            # change the current point of \n",
    "            print(t/dt)\n",
    "            print(int(frac_input*t/dt))\n",
    "            print(int(frac_predicted*t/dt))\n",
    "            fig.data[0].z = inputs_images[min(int(frac_input*t/dt), N_max_input)]\n",
    "            fig.data[1].z = reconstructed_images[min(int(frac_predicted*t/dt), N_max_predicted)]\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim (300, 3, 28, 28) (300, 28, 28)\n",
      "Extract Gray (300, 1, 28, 28) (300, 1, 28, 28)\n",
      "Gray to rgb for gd_images (300, 28, 28, 3) (300, 1, 28, 28)\n",
      "Gray to rgb for reconstructed (300, 28, 28, 3) (300, 28, 28, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28472a88e6e34595bd61cd36fa71dd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.4983388704318937, description='t', max=0.9966777408637874, step=0.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a70434ef429431dbacd5ac1590aab28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'name': 'true image',\n",
       "              'type': 'image',\n",
       "              'uid': '549aea…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = generate_interactive_plot(0, conv_ode, 0, getter, N_frames - Num_pos_velocity, dt, root=None, name=None)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
